{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install pyarrow\n! pip install PyPDF2\n!apt-get install poppler-utils \n!pip install sentencepiece\n!pip install transformers\nimport pandas as pd\nimport json\nfrom transformers import BertForTokenClassification, BertConfig\nfrom transformers import get_scheduler\nfrom tqdm.auto import tqdm\nfrom transformers import AdamW\nimport torch\nimport pytesseract #python version for tesseract \nfrom pdf2image import convert_from_path \n#Warnings\nimport PyPDF2\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nimport re\nimport logging\nimport numpy as np\nfrom tqdm import trange\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n#from seqeval.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport argparse\nimport torch\nfrom transformers import BertForTokenClassification, BertTokenizerFast, DistilBertTokenizer, TFDistilBertForTokenClassification,DistilBertTokenizer, T5Config,DistilBertForTokenClassification\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import Adam\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-06T20:16:42.928895Z","iopub.execute_input":"2022-05-06T20:16:42.929161Z","iopub.status.idle":"2022-05-06T20:17:26.823847Z","shell.execute_reply.started":"2022-05-06T20:16:42.929133Z","shell.execute_reply":"2022-05-06T20:17:26.823089Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Lets download tesseract OCR for character recognisation in resumes","metadata":{}},{"cell_type":"code","source":"class Tesseracttext:\n    def __init__(self,path):\n        self.path=path\n    def _pdftoimg(self):\n        pages=convert_from_path(self.path,500)\n        \n        return pages\n    def _imgtotx(self):\n        pages=self._pdftoimg()\n        for num,page in enumerate(pages):\n            try:\n                \n                   text = pytesseract.image_to_string(page,lang='eng',timeout=60)\n                   print(text)\n            except RuntimeError as timeout_error:\n    # Tesseract processing is terminated\n                   pass\n            #image_to_boxes\n        \n        \ns=Tesseracttext('../input/my-resume/vivek_l_alex_resume.pdf')\ns._imgtotx()  ","metadata":{"execution":{"iopub.status.busy":"2022-05-06T20:17:26.825481Z","iopub.execute_input":"2022-05-06T20:17:26.825731Z","iopub.status.idle":"2022-05-06T20:17:51.948458Z","shell.execute_reply.started":"2022-05-06T20:17:26.825696Z","shell.execute_reply":"2022-05-06T20:17:51.947535Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### NER dataset preprocessing","metadata":{}},{"cell_type":"code","source":"tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\", \"Skills\", \"College Name\", \"Email Address\",\n             \"Designation\", \"Companies worked at\", \"Graduation Year\", \"Years of Experience\", \"Location\"]\n\ntag2idx = {t: i for i, t in enumerate(tags_vals)}\nidx2tag = {i: t for i, t in enumerate(tags_vals)}\n\nMAX_LEN = 500\nEPOCHS = 10\nMAX_GRAD_NORM = 1.0\nMODEL_NAME = 'bert-base-uncased'\nTOKENIZER = BertTokenizerFast('/content/Resume-NER/vocab/vocab.txt', lowercase=True)\ndef convert_goldparse(dataturks_JSON_FilePath):\n    try:\n        training_data = []\n        lines = []\n        with open(dataturks_JSON_FilePath, 'r') as f:\n            lines = f.readlines()\n\n        for line in lines:\n            data = json.loads(line)\n            text = data['content'].replace(\"\\n\", \" \")\n            entities = []\n            data_annotations = data['annotation']\n            if data_annotations is not None:\n                for annotation in data_annotations:\n                    point = annotation['points'][0]\n                    labels = annotation['label']\n                    if not isinstance(labels, list):\n                        labels = [labels]\n\n                    for label in labels:\n                        point_start = point['start']\n                        point_end = point['end']\n                        point_text = point['text']\n\n                        lstrip_diff = len(point_text) - \\\n                            len(point_text.lstrip())\n                        rstrip_diff = len(point_text) - \\\n                            len(point_text.rstrip())\n                        if lstrip_diff != 0:\n                            point_start = point_start + lstrip_diff\n                        if rstrip_diff != 0:\n                            point_end = point_end - rstrip_diff\n                        entities.append((point_start, point_end + 1, label))\n            training_data.append((text, {\"entities\": entities}))\n        return training_data\n    except Exception as e:\n        logging.exception(\"Unable to process \" +\n                          dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n        return None\ndef trim_entity_spans(data: list) -> list:\n    \"\"\"Removes leading and trailing white spaces from entity spans.\n\n    Args:\n        data (list): The data to be cleaned in spaCy JSON format.\n\n    Returns:\n        list: The cleaned data.\n    \"\"\"\n    invalid_span_tokens = re.compile(r'\\s')\n\n    cleaned_data = []\n    for text, annotations in data:\n        entities = annotations['entities']\n        valid_entities = []\n        for start, end, label in entities:\n            valid_start = start\n            valid_end = end\n            while valid_start < len(text) and invalid_span_tokens.match(\n                    text[valid_start]):\n                valid_start += 1\n            while valid_end > 1 and invalid_span_tokens.match(\n                    text[valid_end - 1]):\n                valid_end -= 1\n            valid_entities.append([valid_start, valid_end, label])\n        cleaned_data.append([text, {'entities': valid_entities}])\n    return cleaned_data\n\nsd= convert_goldparse('/content/Resume-NER/data/Resumes.json')\nsd1=trim_entity_spans(sd)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T06:14:31.694457Z","iopub.execute_input":"2022-05-02T06:14:31.694905Z","iopub.status.idle":"2022-05-02T06:14:31.761448Z","shell.execute_reply.started":"2022-05-02T06:14:31.694862Z","shell.execute_reply":"2022-05-02T06:14:31.76062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def id_maker(input,tokenizer, tag2idx, max_len=500):\n    tokenizer=tokenizer(input[0],padding='max_length',return_offsets_mapping=True, max_length=max_len, truncation=True, return_tensors=\"pt\")\n    labels=input[1]['entities']\n    labels.reverse()\n    orig_label=[]\n    lab=[]\n   #padding_length = max_len - len()\n    def offs(off,labels):\n                  \n    \n       if (int(off[0]) == 0 and int(off[1]) == 0):\n         return 'UNKNOWN'\n      \n      \n       for label in labels:\n        \n             if (int(off[1])<=label[1] and int(off[0])>=label[0]) :\n                return label[2]\n      \n       return 'O'\n    for off in tokenizer['offset_mapping'][0]:\n    \n        a=offs(off,labels)\n        orig_label.append(a)\n        lab.append(tag2idx[a])\n    return tokenizer,orig_label,lab   \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=[]\nfor ai in range(len(sd1)) :\n  tokenizer,orig_label,lab= id_maker(sd1[ai],TOKENIZER,tag2idx)\n  labels.append(lab)\n\ntrain_x=[]\nfor i in range(len(sd1)):\n  train_x.append(sd1[i][0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch as th\n\nclass GSMDataset(th.utils.data.Dataset):\n    def __init__(self, tokenizer,x_train,y_train):\n        self.lens=len(x_train)\n        self.sent = x_train\n        self.ent = y_train\n        self.sent = tokenizer(self.sent,padding='max_length',return_offsets_mapping=True, max_length=500, truncation=True, return_tensors=\"pt\")\n        self.ent =  [-100 if x==0 else x for x in self.ent]\n        \n    def __len__(self):\n        return self.lens\n\n    def __getitem__(self, idx):\n        sent_tokens = self.sent[\"input_ids\"][idx]\n        sent_att=self.sent[\"attention_mask\"][idx]\n        ent_tokens = self.ent[idx]\n        ent_tokens = [-100 if x==0 else x for x in ent_tokens] \n        sn_tokens = th.tensor(sent_tokens)\n        sn_att = th.tensor(sent_att)\n        en_tokens=th.tensor(ent_tokens)\n        \n        return dict(input_ids= sn_tokens, attention_mask=sn_att,labels=en_tokens)\n\ntr=GSMDataset(TOKENIZER,train_x,labels)\n#ts=GSMDataset(TOKENIZER,x_test,y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training","metadata":{}},{"cell_type":"code","source":"config = BertConfig.from_pretrained(\"bert-base-uncased\")\nmodel = BertForTokenClassification.from_pretrained( \"bert-base-uncased\",num_labels=len(tag2idx))\nmodel.train()\ntrain_loader = DataLoader(tr, batch_size=16, shuffle=True)\noptim = AdamW(model.parameters(), lr=1e-5)\nnum_epochs = 20\nnum_training_steps = num_epochs * len(train_loader)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optim,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n\npbar = tqdm(range(num_training_steps))\nfor epoch in range(num_epochs):\n        for batch in train_loader:\n            optim.zero_grad()\n            b_input_ids,b_attention_mask,b_labels= batch.items()\n            outputs = model(input_ids=batch[\"input_ids\"],attention_mask=batch[\"attention_mask\"], labels=torch.tensor(batch[\"labels\"]))\n            loss = outputs[0]\n            loss.backward()\n            optim.step()\n            lr_scheduler.step()\n            pbar.update(1)\n            pbar.set_description(f\"train_loss: {loss.item():.5f}\")\n            model.save_pretrained(\"model_ckpts/\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_path = \"/content/model_ckpts\"\nmodel =  BertForTokenClassification.from_pretrained(model_path,num_labels=len(tag2idx)\nresume=\"vivek l alex Web Developer Final year B.Tech student with 3+ years of experience in building web applications, college projects, freelancing, and contributing toOpen Source Softwares. JSS Boys Hostel, C Block,Sector - 62, Noida.(+91) 950000000 vivek61@gmail.com  EXPERIENCE 1mg —  Software Engineer Intern Jan 2020 - Present Avanti Learning Centres — Data Visualization Analyst Jul 2019 - Aug 2019 Responsibilities Task 1 - Report card generatorT ask 2 - Automate student summary generation in bulkMotion Invite — UI/UX & Front-end Developer Jan 2018 - Mar 2018 ResponsibilitiesTask 1 - Redesign the existed websiteTask 2 - Develop the redesigned versionEdcams — Front-end Developer Nov 2017 - Jan 2018 Responsibilities Task 1 - Fix the bugsTask 2 - Develop a new sign up portal EDUCATION cochin college of ebgineering , Noida — BTech 2016 - Present  ●  Average Percentage - 74.20% ● Coursework included○ Algorithms○ Data Structures○ SQL based DBMS○ Turing Machines○ OS Concepts● Have taken hands-on workshops on○ C/C++ Programming○ Designing (UI/UX & Photoshop)○ Web Development (HTML & CSS)SKILLS Frontend:ReactJS, Gatsby, jQuery, JavaScript,HTML, CSS, Materialize, Bootstrap,BulmaBackend: FlaskData Visualization: D3, MatplotlibData Analysis: PandasCloud Suite: Google Cloud PlatformProgramming Languages: C++, C,PythonDatabase: Oracle SQLData Structures and Algorithms in C++ Agile/Scrum and Design thinking approaches ACHIEVEMENTS Special mention at DevFestNoida-18 for FitMo (React-basednative application gamifying themotivation towards fitness)Among top 10 national finalists atSocial Track organized byECell-IITKSecond runner-up at B-Plancompetition organized by ATTACNG\")\n\ninputs = TOKENIZER(resume,padding='max_length',max_length=500, truncation=True, return_tensors=\"pt\")\ninputs_off = TOKENIZER(resume,padding='max_length',return_offsets_mapping=True, max_length=500, truncation=True, return_tensors=\"pt\")\n#predicitng\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\npredicted_token_class_ids = logits.argmax(-1)\n\nentities=predicted_token_class_ids\nentities=[idx2tag[t.item()] for t in predicted_token_class_ids[0]]\ntokenized_text=[resume[int(of[0]):int(of[1])] for of in list(inputs_off.offset_mapping[0])]\nlocation_entity=list(inputs_off.offset_mapping[0])\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### JSON File Creator","metadata":{}},{"cell_type":"code","source":"\n\ntuple_entity_words=list(zip(entities,location_entity))\ntemp_list=[]\ntemp_list_2=[]\nlist_similar_entity=[]\nlist_similar_entity_loc=[]\n#converting offset  maps to text\nfor i in  range(len(tuple_entity_words)):\n  if i != range(len(tuple_entity_words))[-1]:\n\n     if tuple_entity_words[i][0] == tuple_entity_words[i+1][0]:\n       temp_list.append( tuple_entity_words[i][0])\n       temp_list_2.append(tuple_entity_words[i][1])\n     else:\n       temp_list.append(tuple_entity_words[i][0])\n       temp_list_2.append(list(tuple_entity_words[i][1])) \n       list_similar_entity.append(temp_list)\n       list_similar_entity_loc.append(temp_list_2)\n       temp_list=[]\n       temp_list_2=[]\n    \n\n\nent_list=[]\n#json  making \nfor i in range(len(list_similar_entity_loc)):\n  ent=list_similar_entity[i][0]\n\n  start=list_similar_entity_loc[i][0][0]\n  end=list_similar_entity_loc[i][-1][1]\n  text=resume[start:end]\n  if ent !=\"O\" and text != '':\n     ent_list.append({ent:text})\n\nout_file = open(\"resume.json\", \"w\") \njson.dump(ent_list, out_file, indent = 6) \nout_file.close() \n","metadata":{},"execution_count":null,"outputs":[]}]}